# 梯度消失与梯度爆炸

**参考文献**
详解深度学习中的梯度消失、爆炸原因及其解决方法 https://zhuanlan.zhihu.com/p/33006526

全连接网络中由于链式法则需要对激活函数不断求导并累乘，如果这个值始终小于0则产生梯度消失，始终大于0则产生梯度爆炸。
对于sigmoid来说，其导数值恒小于0.25，tanh导数值恒小于1，故在深度神经网络中易产生梯度消失现象，故深度网络更适合Relu、LeakRelu、elu等激活函数
梯度修剪、权重正则可以减轻部分梯度爆炸的问题，Normalization可以减轻梯度消失的问题（将神经元输出强行拉入到敏感区），残差通过短路机制无损传播梯度，LSTM通过记忆保留梯度
