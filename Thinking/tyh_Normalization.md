# Normalization的作用及各种方法的不同

**参考文献**

详解深度学习中的Normalization，BN/LN/WN https://zhuanlan.zhihu.com/p/33173246

-----------------------------分割线------------------------------------

**为什么需要Normalization？**

机器学习、深度学习依赖于一个假设：训练集和测试集处于同一分布之中。而在神经网络中，对于各隐藏层的输出，由于它们经过了层内各种操作，其分布已于各层的输入信号明显不同，且差异随着深度增加而变大。
这导致了每个神经元的输入数据不再同分布：
1. 上层参数需要不断适应新的输入数据分布，降低学习速度。

2. 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。

3. 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。

**Normalization的通用公式**

首先做平移和伸缩变换，将分布强行拉到标准正态分布。然后再平移和再缩放，保证模型的表达能力不因规范化而下降。

规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。

再平移和再缩放的参数由Norm层自己习得，它们能去除与下层计算的密切耦合，其很容易通过梯度下降来学习，简化了神经网络的训练。

**各种Normalization**

Batch Normalization：对Mini-Batch在一个神经元的输出做规范化，BN可看作一种纵向规范化。由于BN依赖于计算Mini-Batch的一阶和二阶统计量（期望和方差），
故每个Mini-Batch的分布应尽可能与整体数据是同分布的。BN适合于Batch Size较大、数据分布比较接近，shuffle做得比较好得情况。

Layer Normalization：针对单个样本，一层中的所有神经元进行Normalization，不再受Mini-Batch数据分布的影响，但输入特征差距很大的话，如颜色和大小，则LN的处理可能降低模型的表达能力。
LN特别适合用于自然语言处理当中。


**Transformer中Normalization的思考**

Transformer中为何不使用Batch Norm而使用Layer Norm？每个Batch中存在不同的句子，这些句子不一定在同一段中，也不一定在同一篇文章中，
直观上思考，这些句子本就处于不同的分布上，强行拉在同一分布中则会损失不同句子之前的差异信息。在我们之前的讨论中，Batch Norm依赖于Mini-Batch的一阶和二阶统计量（期望和方差）
故适合用于每个Mini-Batch与整体数据处于相同分布的情况下。
而已有文章[Rethinking Batch Normalization in Transformers](https://arxiv.org/pdf/2003.07845.pdf)证明NLP中使用Batch Norm的均值和方差一直振荡，
导致其提供的梯度也具有不稳定性，模型难以收敛。
同样还有一篇文章[Understanding and Improving Layer Normalization](https://papers.nips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)发现不做Rescale反而会有效果提升，
而Layer Norm起作用的原因在于既使得输入的分布更加稳定，同时使后向的梯度也更加稳定，二者共同产生Layer Norm的优化效果

**回答思路**
层内操作-->分布不同-->失去独立同分布-->三个问题：上层艰难适应下层，饱和区，慎重更新策略-->norm 先平移伸缩拉至标准正态分布，降低上层的学习难度，同时拉至非饱和区，减缓梯度消失，随后再平移再伸缩保证模型的表达能力，这一步与层内操作解耦，仅产生小偏移，不过分影响拉至标准正态分布的效果-->BN和LN-->起作用原因：权重偏移不变性，数据偏移不变性
