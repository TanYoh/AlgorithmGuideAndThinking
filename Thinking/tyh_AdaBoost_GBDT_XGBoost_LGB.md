# Difference Between AdaBoost & GBDT & XGBoost & LGB

**参考文章**

Adaboost, GBDT 与 XGBoost 的区别 https://zhuanlan.zhihu.com/p/42740654

GBDT与XGBoost区别 https://www.zhihu.com/question/41354392/answer/98658997

为何XGBoost二阶展开后效果更好？https://www.zhihu.com/question/277638585/answer/522272201

拟牛顿法相比梯度下降法更快的直观解释 https://www.zhihu.com/question/19723347

XGBoost与LGB的详细分析 https://zhuanlan.zhihu.com/p/87885678

-----------------------------分割线------------------------------------

AdaBoost(adaptive boosting，自适应) 每个新的模型都会基于前一个模型的表现结果进行调整。

AdaBoost可看作GBDT的一个特例（损失函数的设置）

和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。
不同的是，AdaBoost 是通过提升错分数据点的权重来定位模型的不足而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。
因此相比 AdaBoost使用指数函数作为损失函数, Gradient Boosting 可以使用更多种类的目标函数。

根据奥卡姆剃刀原则，如果GBDT和线性回归或逻辑回归在某个问题上表现接近，那么我们应该选择相对比较简单的线性回归或逻辑回归。

-----------------------------分割线------------------------------------

传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，若使用线性分类器，XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。

为何XGBoost使用了二阶泰勒展开？对于MSE展开是二阶项+一介项的形式，而像logloss则没有这样的形式，损失函数的梯度并不总是容易求得
而为了获得统一的形式，则采用泰勒展开来获得二阶项，这样可以把MSE的推导直接复用到其他损失函数（可从推导中看出）。
而且二阶信息能让梯度收敛更加准确，一阶指引梯度方向，二阶指引梯度方向如何变化。

XGBoost在代价函数里**显式**加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。
从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。

Shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。
实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（该点在GBDT中亦有实现，可忽略）

列抽样（column subsampling）。XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。

对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。
这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。
当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

XGBoost代价函数加入正则项是否优CART的剪枝？CART的剪枝是用分裂后的某种值 - 分裂前的某种值，从而得到增益。
而在XGBoost当中，只有增益大于阈值（正则项里叶子节点数T的系数）时才让节点分裂，相当于预剪枝。

**总结**来说：XGBoost与GBDT的区别可分为两个方面：性能优化和工程优化

**性能优化**

1. 对损失函数的二阶泰勒展开，速度更快，方向更准，统一形式
2. 显式加入正则项（对叶子数目的L1正则和对叶子权重的L2正则），防止过拟合，降低了模型的varience
3. 列抽样、行抽样，减少计算，降低弱分类器之间的相关性，降低模型varience，减少过拟合
4. 缺失值自动学习出分裂方向（稀疏感知）
5. 可自由选择弱分类器

**工程优化**
1. 候选分位点划分，根据特征分布分位数提出候选切分点（加权分位，二阶导为样本权重），分为Global和Local，Global在学习一棵树前提出，Local在每次分裂之后提出新的划分点（因为切割节点分位发生了变化）。
2. 稀疏感知算法：只考虑非缺失值的遍历，缺失值根据增益划分到左右节点
3. 块结构设计，对特征进行预排序，保存为block结构并反复使用，建立特征--样本梯度统计值的索引，降低计算量，同时存储结构相互独立，计算最大增益时可并行计算。
4. 缓存访问优化算法，特征--梯度索引可能存在访问操作的内存空间不连续，为此给每个线程分配一段连续的缓存区，将需要的梯度信息存储在缓存区中
5. “核外”块运算，独立线程从硬盘中读入数据，块压缩（对列）、块拆分（块存储到不同磁盘）
6. 大量并行操作，如上讨论

-----------------------------分割线------------------------------------

**LGB相对于XGBoost的优化**

1. 单边梯度抽样算法：对梯度大样本进行更多的关注，随机抽样梯度小的样本，极大减小计算量，但需要对梯度小的样本乘以权重以减小采样对原始数据分布造成的影响
2. 直方图算法：将连续特征离散化为离散特征（与XGBoost加权分位不同在于LGB对于特征本身进行了修改），内存占用更小，计算更快，可能影响模型精度，但降低了varience；
构建直方图时通过父节点减相邻叶节点直方图的方式构建，减少了一半计算量；仅用非零特征构建直方图
4. 互斥特征捆绑算法：将一些特征根据互斥程度融合绑定，降低特征数量，哪些特征绑定取决于互斥程度
5. XGBoost中采用Level-wise的增长策略（可并行），LGB采用Leaf-wise的增长策略，减少了计算量，配合最大深度防止过拟合，但无法并行分裂
6. **决策树不适合用one-hot编码**，故LGB采用many vs many的方式，将类别特征分为两个子集，实现类别特征的最优切分
7. 特征并行上，LGB不进行数据垂直划分，每个机器具有训练集完整数据（和直方图算法有关），降低通信消耗
8. 数据并行上，LGB采用分散规约，将直方图整合任务分摊到不同机器上，通过直方图做差整合全局直方图，降低通信消耗
9. 投票并行，只合并通过投票得到部分特征的直方图从而达到降低通信量的目的
10. 缓存优化上，由于直方图算法不再需要XGBoost的特征--梯度索引，大大降低了缓存消耗

