# Difference Between AdaBoost & GBDT & XGBoost & LGB

**参考文章**

Adaboost, GBDT 与 XGBoost 的区别 https://zhuanlan.zhihu.com/p/42740654

GBDT 与 XGBoost区别 https://www.zhihu.com/question/41354392/answer/98658997

为何XGBoost二阶展开后效果更好？https://www.zhihu.com/question/277638585/answer/522272201

拟牛顿法相比梯度下降法更快的直观解释 https://www.zhihu.com/question/19723347

AdaBoost(adaptive boosting，自适应) 每个新的模型都会基于前一个模型的表现结果进行调整。

AdaBoost可看作GBDT的一个特例（损失函数的设置）

和 AdaBoost 一样，Gradient Boosting 也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。
不同的是，AdaBoost 是通过提升错分数据点的权重来定位模型的不足而 Gradient Boosting 是通过算梯度（gradient）来定位模型的不足。
因此相比 AdaBoost使用指数函数作为损失函数, Gradient Boosting 可以使用更多种类的目标函数。

根据奥卡姆剃刀原则，如果GBDT和线性回归或逻辑回归在某个问题上表现接近，那么我们应该选择相对比较简单的线性回归或逻辑回归。

传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，若使用线性分类器，XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。

为何XGBoost使用了二阶泰勒展开？对于MSE展开是二阶项+一介项的形式，而像logloss则没有这样的形式，损失函数的梯度并不总是容易求得
而为了获得统一的形式，则采用泰勒展开来获得二阶项，这样可以把MSE的推导直接复用到其他损失函数。
而且二阶信息能让梯度收敛更加准确，一阶指引梯度方向，二阶指引梯度方向如何变化。

XGBoost在代价函数里**显式**加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。
从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。

Shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。
实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（该点在GBDT中亦有实现，可忽略）

列抽样（column subsampling）。XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。

对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。

XGBoost的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。
这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。
当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

XGBoost代价函数加入正则项是否优CART的剪枝？CART的剪枝是用分裂后的某种值 - 分裂前的某种值，从而得到增益。
而在XGBoost当中，只有增益大于阈值（正则项里叶子节点数T的系数）时才让节点分裂，相当于预剪枝。









