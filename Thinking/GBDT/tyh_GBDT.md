# GBDT

## 梯度

当前损失函数L(yi, F(x))对树F(xi)的梯度

## 各损失函数的比较

在GBDT中，CART自身的损失函数是确定的，对于回归是平方损失函数，但 Boosting 求前轮预测与实际值之前的损失函数则是可以自由选定的，前提是可以一阶可导！

square loss（MSE）的优点是便于理解和实现，缺点在于对于异常值它的鲁棒性较差。
一个异常值造成的损失由于二次幂而被过分放大，会影响到最后得到模型在测试集上的表现。

absolute loss（MAE）指目标变量和预测变量之间差异绝对值之和，使用绝对误差对离群点更加鲁棒。

对所有的观测数据，如果我们只给一个预测结果来最小化MSE，那么该预测值应该是所有目标值的均值。
但是如果我们试图最小化MAE，那么这个预测就是所有目标值的中位数。我们知道中位数对于离群点比平均值更鲁棒，这使得MAE比MSE更加鲁棒。

使用MAE损失（特别是对于神经网络）的一个大问题是它的梯度始终是相同的，这意味着即使对于小的损失值，其梯度也是大的，并且导数不连续，其求解效率很低。
MSE在这种情况下的表现很好，即使采用固定的学习率也会收敛。MSE损失的梯度在损失值较高时会比较大，随着损失接近0时而下降，从而使其在训练结束时更加精确。

为了平衡二者的优劣势则可选择Huber Loss
当absolute loss大于阈值delta时采用MAE，小于该阈值时采用MSE，delta的选择表明什么是离群点，故其主要问题在于需要迭代训练delta。

Log-cosh Loss，相比MSE更加平滑，相比Huber函数处处可导，适合于XGBoost，但仍然存在梯度和Hessian问题，对于误差很大的预测，其梯度和hessian是恒定的。因此会导致XGBoost中没有分裂。

## GBDT的分类

GBDT原始运用在回归模型当中，若运用在分类模型中需要改成softmax + Boosting的形式。每一次迭代需要构建类别数目棵树，所有树的结果进入softmax，输出每个类别的概率，然后拟合概率的负梯度。
一般分类使用交叉熵作为单样本的损失函数，其负梯度为真实标签与预测概率之差。
