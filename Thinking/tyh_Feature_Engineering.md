# Feature Engineering

**参考文献**
机器学习中，有哪些特征选择的工程方法？ https://www.zhihu.com/question/28641663/answer/110165221

PCA https://zhuanlan.zhihu.com/p/77151308

--------------------------------分割线------------------------------------

整个特征工程从开始选择特征到建立模型后的特征监测分为几个部分：理解业务、特征获取、预处理（数据清洗、特征选择）、特征监控

**理解业务**

理解业务是开始，首先目标是什么，搞明白需要什么数据，根据领域知识尽可能选出相关的特征

**特征获取**

理解业务之后，需要知道怎样去搜集想得到的数据，以及数据的存储方式

**数据清洗**

数据预处理的开始，主要目的为分布均匀化、极端变量一般化，分为几个部分：无量纲化、定量特征二值化、定性特征哑编码、缺失值的处理、数据变换等

无量纲化：

标准化（均值0，方差1），归一化（区间缩放法、0-1），均值归一化（x-mean,-1-1）

定量特征二值化、区间量化（分箱、固定宽度、分位数）：

减少模型的学习难度，忽略不必要的噪声，包括target本身也可二值化、区间量化，取决于目标

定性特征哑编码：

哑编码与one-hot encoding区别：哑编码可以为全0，可以引入非线性，对于衡量距离的模型来说有提升，但不适用于决策树模型

缺失值处理：

分类：完全随机缺失、随机确实和完全非随机缺失

删除（仅适用于缺失值较少的情况）、特殊值填充、均值填充（连续值）、众数填充（离散值）、前向填充、分类填充、线性插值、多重插值（抽样插补值产生m个数据集、统计分析结果、根据评分函数进行选择）

数据变换：

多项式变换（交互特征..如area = width * length），对数变换（适合于某些值太大..或者特征分布类似于长尾分布）


**特征选择**

Filter：

1. 依赖于方差，方差较小的特征意义不大：方差选择法

2. 依赖于相关性，留下与target相关性较大的特征：计算pearson系数、speaman系数、卡方检验（定性自变量与因变量的相关性）、互信息法

Wrapper：

1. 迭代消除特征---建立基模型，每轮训练后消除若干权值系数的特征

Embedded：

1. 基于惩罚项的特征选择（L1、L2（L2相同，但L1为0，则平分））
2. 基于树的特征选择

降维：

PCA（无监督，提高特征发散能力）、LDA（有监督，提高特征分类能力）

PCA 简单来说是求特征矩阵的协方差矩阵的特征值和特征向量使其对角化，降低特征之间的相关性，提高特征内部的方差，取前k大特征值对应的特征向量构建矩阵，使样本与其相乘，线性变换至低维空间

描述：降维是为了将特征矩阵线性变换到低维空间，同时尽可能地使降维后的特征方差尽可能大，之间的相关性尽可能小，也即零均值化后的协方差尽可能为0。
为此，先构建协方差矩阵C（对角线上为方差，其余为协方差，为实对称矩阵），目标则是将其线性变化成对角线按元素大小从上至下排列，而其他非对角线为0，也即对角化。
由于n\*n实对称矩阵的性质，其一定可以找到n个单位正交特征向量，而特征向量矩阵的转置乘以可对角化的矩阵再乘以特征向量矩阵即可实现对角化。故问题转化为求协方差矩阵的特征值和特征向量。
最后取前k大特征值对应的特征向量组成矩阵去与原始数据线性变换，即可实现降维。

PCA 可以缓解维度灾难，降低噪声（但也可能导致过拟合），特征独立

SGD

LDA 

**特征监控**

模型运行后，数据地变化可能导致特征重要性的变化，所以一个是降低因特征强相关产生的过拟合，另一个就需要比较好的可解释性，知道什么特征出了问题，及时处理
